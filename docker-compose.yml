---
x-vllm-env: &vllm-env
  HF_HUB_ENABLE_HF_TRANSFER: "1"
  # Set a shared API key for all vLLM servers
  VLLM_API_KEY: "${VLLM_API_KEY:-changeme}"
  # Optional Hugging Face token for gated models (e.g., Meta Llama 3.1)
  HF_TOKEN: "${HF_TOKEN:-}"
  HUGGING_FACE_HUB_TOKEN: "${HF_TOKEN:-}"

x-vllm-common: &vllm-common
  image: vllm/vllm-openai:latest
  ipc: host
  runtime: nvidia
  environment:
    <<: *vllm-env
  command: >
    --host 0.0.0.0
    --download-dir /models/.cache
    --swap-space 16
    --enable-prefix-caching
    --api-key ${VLLM_API_KEY}
  volumes:
    - /srv/llm/.cache:/models/.cache
  # If you want to serve local folders from /srv/llm/weights, also mount:
  #   - /srv/llm/weights:/models

services:
  # --- Dev/agents: fast coder (7B 8-bit) ---
  coder-fast:
    container_name: coder-fast
    <<: *vllm-common
    restart: "no"
    command: >
      --model Qwen/Qwen2.5-Coder-7B-Instruct
      --quantization bitsandbytes
      --served-model-name coder-fast
      --port 8001
      --dtype auto
      --gpu-memory-utilization 0.9
      --max-model-len 4096
      --max-num-seqs 4
    ports: ["8001:8001"]
    profiles: ["coder"]

  # --- Chat/general assistant: Llama 3.1 8B (8-bit) ---
  chat-general:
    container_name: chat-general
    <<: *vllm-common
    restart: "no"
    command: >
      --model meta-llama/Meta-Llama-3.1-8B-Instruct
      --quantization bitsandbytes
      --served-model-name chat-general
      --port 8002
      --dtype auto
      --gpu-memory-utilization 0.9
      --max-model-len 8192
      --max-num-seqs 4
    ports: ["8002:8002"]
    profiles: ["chat"]

  # --- General reasoning / math (7B 8-bit) ---
  general-reasoner:
    container_name: general-reasoner
    <<: *vllm-common
    restart: "no"
    command: >
      --model deepseek-ai/DeepSeek-R1-Distill-Qwen-7B
      --quantization bitsandbytes
      --served-model-name general-reasoner
      --port 8003
      --dtype auto
      --gpu-memory-utilization 0.9
      --max-model-len 4096
      --max-num-seqs 3
    ports: ["8003:8003"]
    profiles: ["general"]

  # --- Deep coding focus (16B AWQ, slower but higher quality) ---
  coder-slow:
    container_name: coder-slow
    <<: *vllm-common
    restart: "no"
    command: >
      --model TechxGenus/DeepSeek-Coder-V2-Lite-Instruct-AWQ
      --quantization awq
      --served-model-name coder-slow
      --port 8004
      --dtype auto
      --gpu-memory-utilization 0.9
      --max-model-len 4096
      --max-num-seqs 2
    ports: ["8004:8004"]
    profiles: ["coderslow"]

  # --- Tool-capable agent (Qwen2.5 7B Instruct, 8-bit) ---
  agent-tools:
    container_name: agent-tools
    <<: *vllm-common
    restart: "no"
    command: >
      --model Qwen/Qwen2.5-7B-Instruct
      --quantization bitsandbytes
      --served-model-name agent-tools
      --port 8005
      --dtype auto
      --gpu-memory-utilization 0.9
      --max-model-len 4096
      --max-num-seqs 3
    ports: ["8005:8005"]
    profiles: ["agent"]

  # --- Vision diffusion server (Stable Diffusion 3.5 Medium, FastAPI wrapper) ---
  vision-diffusion:
    container_name: vision-diffusion
    build: ./vision-server
    image: vision-diffusion:latest
    runtime: nvidia
    restart: "no"
    environment:
      MODEL_ID: "${VISION_MODEL_ID:-stabilityai/stable-diffusion-3.5-medium}"
      MODEL_REVISION: "${VISION_MODEL_REVISION:-}"
      HF_TOKEN: "${HF_TOKEN:-}"
      ENABLE_SAFETY_CHECKER: "${VISION_ENABLE_SAFETY_CHECKER:-0}"
      ENABLE_TILING: "${VISION_ENABLE_TILING:-1}"
      ENABLE_ATTENTION_SLICING: "${VISION_ENABLE_ATTENTION_SLICING:-1}"
      MAX_EDGE: "${VISION_MAX_EDGE:-1024}"
    volumes:
      - /srv/llm/.cache/vision:/models
    ports: ["8188:8188"]
    profiles: ["vision"]

  # --- Open WebUI front-end for the stack ---
  open-webui:
    container_name: open-webui
    image: ghcr.io/open-webui/open-webui:main
    restart: unless-stopped
    profiles: ["launcher"]
    environment:
      WEBUI_AUTH: "${OPEN_WEBUI_AUTH:-true}"
      OPENAI_API_BASE_URL: "http://launcher:8000/chat/v1"
      OPENAI_API_KEY: "${VLLM_API_KEY:-changeme}"
      ENABLE_OPENAI_API: "true"
    ports: ["3000:8080"]
    volumes:
      - open-webui-data:/app/backend/data
    depends_on: [launcher]

  # --- Local SearXNG meta search engine ---
  searxng:
    container_name: searxng
    image: searxng/searxng
    restart: unless-stopped
    profiles: ["launcher"]
    environment:
      BASE_URL: "http://searxng:8080/"
    volumes:
      - ./searxng:/etc/searxng
    ports:
      - "8080:8080"
    depends_on: [open-webui]

  # --- Single front door on :8000 with path routing ---
  launcher:
    build: ./launcher
    image: lazy-launcher:latest
    container_name: launcher
    restart: unless-stopped
    profiles: ["launcher"]
    environment:
      MAP__coder__container: "coder-fast"
      MAP__coder__port: "8001"
      MAP__chat__container: "chat-general"
      MAP__chat__port: "8002"
      MAP__general__container: "general-reasoner"
      MAP__general__port: "8003"
      MAP__coderslow__container: "coder-slow"
      MAP__coderslow__port: "8004"
      MAP__agent__container: "agent-tools"
      MAP__agent__port: "8005"
      MAP__vision__container: "vision-diffusion"
      MAP__vision__port: "8188"
      MAP__vision__health: "/healthz"
      MAP__vision__auth: "passthrough"
      MAP__vision__models: "vision-diffusion,sd35m,sd15"
      IDLE_SECONDS: "600"
      START_TIMEOUT_SECONDS: "120"
      VLLM_API_KEY: "${VLLM_API_KEY}"
    ports:
      - "8000:8000"
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
    depends_on: []   # it can start containers on demand

volumes:
  open-webui-data:

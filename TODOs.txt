# vLLM Launcher + Multi-Model Stack TODO

## Infra & Compose
- [ ] **Pin container names** in `docker-compose.yml` (`container_name: coder-fast|chat-general|general-reasoner|coder-slow|agent-tools|launcher`) so the launcher always finds them.
- [ ] **.env**: add `VLLM_API_KEY=<hex>`, `COMPOSE_PROJECT_NAME=llm`.
- [ ] **Profiles**: ensure `coder`, `chat`, `general`, `coderslow`, `agent`, `vision`, `launcher` profiles exist and run cleanly.
- [ ] **WSL ext4 space** verified on D: (done). Ensure weights+cache use `/srv/llm/.cache` (ext4).
- [ ] **Local web search**: finish wiring the `searxng_search` tool inside Open WebUI (or via a launcher proxy route) so it calls `http://searxng:8080/search` with mode-aware engines.

## Launcher (lazy start/stop proxy)
- [ ] **Path rewrite** confirmed (`/chat|/coder|/general|/coderslow|/agent` → strip prefix → backend `/v1/...`).
- [ ] **Increase start timeout**: `START_TIMEOUT_SECONDS=300`.
- [ ] **Idle shutdown**: `IDLE_SECONDS=1800` (or your taste).
- [ ] **Streaming keep-alive**: refresh `lastHit` on `proxyRes 'data'` (mid-stream timer bump).
- [ ] **Health endpoint**: add `GET /healthz` returning mappings + container states.
- [ ] **Engine API** (optional): replace shelling out to `docker` CLI with HTTP calls to `/var/run/docker.sock`.
- [ ] **Per-route defaults**: env vars to set model name, `max_tokens`, temp, etc., for each prefix.
- [ ] **Basic auth** (optional): add header check for `X-API-Secret` or HTTP Basic before proxying.
- [ ] **Bearer enforcement**: require inbound `Authorization: Bearer $VLLM_API_KEY` in `launcher/server.js` (with opt-out env if absolutely needed) instead of auto-injecting the header when clients omit it.

## Models & VRAM policy
- [ ] **GPU headroom**: set `--gpu-memory-utilization 0.90` (one-at-a-time policy).
- [ ] **coder-fast** flags: `Qwen/Qwen2.5-Coder-7B-Instruct`, `--quantization bitsandbytes`, `--max-model-len 4096`, `--max-num-seqs 4`.
- [ ] **chat-general** flags: `meta-llama/Meta-Llama-3.1-8B-Instruct`, `--quantization bitsandbytes`, `--max-model-len 8192`, `--max-num-seqs 4`.
- [ ] **general-reasoner** flags: `deepseek-ai/DeepSeek-R1-Distill-Qwen-7B`, `--quantization bitsandbytes`, `--max-model-len 4096`, `--max-num-seqs 3`.
- [ ] **coder-slow** flags: `TechxGenus/DeepSeek-Coder-V2-Lite-Instruct-AWQ`, `--quantization awq`, `--max-model-len 4096`, `--max-num-seqs 2`.
- [ ] **agent-tools** flags: `Qwen/Qwen2.5-7B-Instruct`, `--quantization bitsandbytes`, `--max-model-len 4096`, `--max-num-seqs 3`.
- [ ] **vision-diffusion**: Stable Diffusion v1.5 fp16 FastAPI server (`vision-server/`); confirm VRAM fits plus document `MAX_EDGE` guardrails.
- [ ] **Pre-seed HF caches** on ext4 (one-time, add `--token "$HF_TOKEN"` for gated repos):
  ```bash
  huggingface-cli download Qwen/Qwen2.5-Coder-7B-Instruct --local-dir /srv/llm/.cache/models--Qwen--Qwen2.5-Coder-7B-Instruct --local-dir-use-symlinks False --include "*"
  huggingface-cli download meta-llama/Meta-Llama-3.1-8B-Instruct --local-dir /srv/llm/.cache/models--meta-llama--Meta-Llama-3.1-8B-Instruct --local-dir-use-symlinks False --include "*" --token "$HF_TOKEN"
  huggingface-cli download deepseek-ai/DeepSeek-R1-Distill-Qwen-7B --local-dir /srv/llm/.cache/models--deepseek-ai--DeepSeek-R1-Distill-Qwen-7B --local-dir-use-symlinks False --include "*"
  huggingface-cli download TechxGenus/DeepSeek-Coder-V2-Lite-Instruct-AWQ --local-dir /srv/llm/.cache/models--TechxGenus--DeepSeek-Coder-V2-Lite-Instruct-AWQ --local-dir-use-symlinks False --include "*"
  huggingface-cli download Qwen/Qwen2.5-7B-Instruct --local-dir /srv/llm/.cache/models--Qwen--Qwen2.5-7B-Instruct --local-dir-use-symlinks False --include "*"
  ```

## Security & Exposure
- [ ] **Keep vLLM API key static** in `.env`; rotate quarterly.
- [ ] **Port exposure**: keep only **launcher** on `:8000` exposed; model containers internal only.
- [ ] (Optional) **Cloudflare Tunnel** → public HTTPS URL; restrict with Cloudflare Access; pass `Authorization` through.
- [ ] **Windows Defender exclusion** for the WSL VHD folder backing `/srv/llm`.

## DX: scripts & profiles
- [ ] **Switch script** (`/srv/llm/compose/switch.sh`):
  ```bash
  #!/usr/bin/env bash
  set -e
  case "$1" in
    coder)
            docker compose stop chat-general general-reasoner coder-slow agent-tools
            docker compose --profile launcher --profile coder up -d ;;
    chat)
            docker compose stop coder-fast general-reasoner coder-slow agent-tools
            docker compose --profile launcher --profile chat up -d ;;
    general)
            docker compose stop coder-fast chat-general coder-slow agent-tools
            docker compose --profile launcher --profile general up -d ;;
    coderslow)
            docker compose stop coder-fast chat-general general-reasoner agent-tools
            docker compose --profile launcher --profile coderslow up -d ;;
    agent)
            docker compose stop coder-fast chat-general general-reasoner coder-slow
            docker compose --profile launcher --profile agent up -d ;;
    *) echo "usage: $0 {coder|chat|general|coderslow|agent}"; exit 1;;
  esac
  ```
- [ ] **Makefile** (nice to have):
  ```make
  up-coder:     ; docker compose --profile launcher --profile coder     up -d && docker compose stop chat-general general-reasoner coder-slow agent-tools
  up-chat:      ; docker compose --profile launcher --profile chat      up -d && docker compose stop coder-fast general-reasoner coder-slow agent-tools
  up-general:   ; docker compose --profile launcher --profile general   up -d && docker compose stop coder-fast chat-general coder-slow agent-tools
  up-coderslow: ; docker compose --profile launcher --profile coderslow up -d && docker compose stop coder-fast chat-general general-reasoner agent-tools
  up-agent:     ; docker compose --profile launcher --profile agent     up -d && docker compose stop coder-fast chat-general general-reasoner coder-slow
  logs:         ; docker compose logs -f launcher
  ```

## Monitoring & Logs
- [ ] **Launcher logs**: on start/stop/timeout; include measured spin-up duration.
- [ ] **vLLM readiness**: grep for “Uvicorn running” and “Finished loading model” and surface in launcher logs.
- [ ] (Optional) **Node exporter** or `glances` for quick GPU/CPU/NVMe telemetry; or `nvidia-smi dmon`.

## Bench & SLOs
- [ ] **Start-time benchmarks** (warm/cold) scripts you already have.
- [ ] **Throughput probe**: simple tokens/s test (1k-token completion, temp=0.2) to track regressions.
- [ ] **Latency SLO**: fail launcher if backend not healthy within `START_TIMEOUT_SECONDS`; return `503` with `Retry-After`.

## Website chat readiness
- [ ] **Site chat default**: set `/v1/*` to map to `/chat/v1/*` (so future widgets can just use one base).
- [ ] **System prompt** for website safety baked into `chat-general` requests (either at the launcher or upstream when you add your proxy later).
- [ ] **CORS allowlist** (later, when you add a front-end): `theschoonover.net` and `www` subdomain.

## Lifecycle management
- [ ] **Systemd wrapper**: add a unit that starts the launcher and desired model profile, and on stop runs `docker compose stop coder-fast chat-general general-reasoner coder-slow agent-tools` so all LLMs shut down with the launcher.

## Hardening (later but easy wins)
- [ ] **Nginx or Caddy front** (TLS on LAN or for public).  
- [ ] **Rate limiting** per IP / route.  
- [ ] **Per-route API keys** (different key for `/chat` vs `/coder` if you share beyond LAN).  
- [ ] **Log redaction**: strip auth headers before writing access logs.

## Agent hooks (so your coding agent can manage this)
- [ ] **Launcher admin API** (tiny JSON endpoints):
  - `GET /routes` → show route→container map and states.
  - `POST /admin/start {route}` / `POST /admin/stop {route}`.
  - `GET /stats` → spin-up times, request counts, lastHit timestamps.
- [ ] **Health checks** for CI:
  - `curl /coder/v1/models` and `/chat/v1/models` → 200 within X seconds.
- [ ] **Compose validation**: agent task that lints compose, verifies ports free, names stable.
- [ ] **Auto-seed** task: if cache missing, run `huggingface-cli download` before first boot.

## Client configs (ready for copy)
- [x] **Continue (VS Code)** models configuration JSON (`continue.config.json`).

## Sanity commands (for the agent to use)
```bash
# Start launcher only (no models hot)
docker compose up -d launcher

# Prove lazy start works for chat
curl -s http://<ip>:8000/chat/v1/models -H "Authorization: Bearer $VLLM_API_KEY" | jq .

# Tail launcher during a bring-up
docker logs -f launcher

# See container states/names
docker ps --format '{{.Names}}	{{.Status}}'

# Stop everything but launcher
docker compose stop coder-fast chat-general general-reasoner coder-slow agent-tools
```
